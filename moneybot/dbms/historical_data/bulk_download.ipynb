{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import nasdaqdatalink\n",
    "# from requests import Session\n",
    "# from requests_cache import CacheMixin, SQLiteCache\n",
    "# from requests_ratelimiter import LimiterMixin, MemoryQueueBucket\n",
    "# from pyrate_limiter import Duration, RequestRate, Limiter\n",
    "# class CachedLimiterSession(CacheMixin, LimiterMixin, Session):\n",
    "#     pass\n",
    "\n",
    "# session = CachedLimiterSession(\n",
    "#     limiter=Limiter(RequestRate(2, Duration.SECOND*5)),  # max 2 requests per 5 seconds\n",
    "#     bucket_class=MemoryQueueBucket,\n",
    "#     backend=SQLiteCache(\"yfinance.cache\"),\n",
    "# )\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PROXY_SERVER=os.getenv(\"PROXY_SERVER\")\n",
    "api_key=os.getenv(\"NASDAQ_DATA_LINK_API_KEY\")\n",
    "nasdaqdatalink.ApiConfig.api_key = api_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STORE = Path('store/assets.h5')\n",
    "csv_path = Path('store/csv') \n",
    "if not csv_path.exists():\n",
    "    csv_path.mkdir() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Whole Sharadar Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2036625004.py, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 84\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f'Extracted files to {'.'}')\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# enter the Sharadar table you would like to retrieve \n",
    "def display_menu():\n",
    "    options = [\n",
    "        \"1: SHARADAR/TICKERS\",\n",
    "        \"2: SHARADAR/ACTIONS\",\n",
    "        \"3: SHARADAR/DAILY\",\n",
    "        \"4: SHARADAR/SEP\",\n",
    "        \"5: SHARADAR/SP500\",\n",
    "        \"6: SHARADAR/SF1\",  \n",
    "    ]\n",
    "    \n",
    "    print(\"Please select a table:\")\n",
    "    for i, option in enumerate(options, 1):\n",
    "        print(f\"{i}. {option}\")\n",
    "    \n",
    "    choice = input(\"Enter the number of your choice: \")\n",
    "    return int(choice)\n",
    "\n",
    "def select_table():\n",
    "    tbl = ''\n",
    "    choice = display_menu()\n",
    "    if choice == 1:\n",
    "        print(\"Fetching SHARADAR/TICKERS\")\n",
    "        tbl = 'TICKERS'\n",
    "    elif choice == 2:\n",
    "        print(\"Fetching SHARADAR/ACTIONS\")\n",
    "        tbl = 'ACTIONS'\n",
    "    elif choice == 3:\n",
    "        print(\"Fetching SHARADAR/DAILY\")\n",
    "        tbl = 'DAILY'\n",
    "    elif choice == 4:\n",
    "        print(\"Fetching SHARADAR/SEP\")\n",
    "        tbl = 'SEP'\n",
    "    elif choice == 5:\n",
    "        print(\"Fetching SHARADAR/SP500\")\n",
    "        tbl = 'SP500'\n",
    "    elif choice == 6:\n",
    "        print(\"Fetching SHARADAR/SF1\")\n",
    "        tbl = 'SF1'\n",
    "    else:\n",
    "        print(\"Invalid choice, please try again.\")\n",
    "        select_table()\n",
    "    return tbl\n",
    "    \n",
    "\n",
    "table = select_table()\n",
    "\n",
    "destFileRef = f'store/csv/SHARADAR_{table}.csv.zip'\n",
    "# optionally add parameters to the url to filter the data retrieved,\n",
    "#  as described in the associated table's\n",
    "#  documentation, eg here: https://www.quandl.com/databases/SF1/documentation/getting-started\n",
    "url = 'https://www.quandl.com/api/v3/datatables/SHARADAR/%s.json?qopts.export=true&api_key=%s' % (table, api_key) \n",
    "\n",
    "def bulk_fetch(url=url, destFileRef=destFileRef):\n",
    "  version = sys.version.split(' ')[0]\n",
    "  if version < '3':\n",
    "    import urllib2\n",
    "    fn = urllib2.urlopen\n",
    "  else:\n",
    "    import urllib\n",
    "    fn = urllib.request.urlopen\n",
    "\n",
    "  valid = ['fresh','regenerating']\n",
    "  invalid = ['generating']\n",
    "  status = ''\n",
    "  \n",
    "  while status not in valid:\n",
    "    Dict = json.loads(fn(url).read())\n",
    "    last_refreshed_time = Dict['datatable_bulk_download']['datatable']['last_refreshed_time']\n",
    "    status = Dict['datatable_bulk_download']['file']['status']\n",
    "    link = Dict['datatable_bulk_download']['file']['link']\n",
    "    print(status)\n",
    "    if status not in valid:\n",
    "      time.sleep(60)\n",
    "\n",
    "  print('fetching from %s' % link)\n",
    "  zipString = fn(link).read()\n",
    "  f = open(destFileRef, 'wb')\n",
    "  f.write(zipString)\n",
    "  f.close()\n",
    "  print('fetched')\n",
    "  with zipfile.ZipFile(destFileRef, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "        print(f'Extracted files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fresh\n",
      "fetching from https://aws-gis-link-pro-us-east-1-datahub.s3.amazonaws.com/export/SHARADAR/SP500/SHARADAR_SP500_2_b45e10f42373d31ee51964c196004fe0.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAX5EW3SB5PFX5A3TV%2F20240601%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240601T182332Z&X-Amz-Expires=1800&X-Amz-Security-Token=IQoJb3JpZ2luX2VjENv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJIMEYCIQCeUsR%2FhUvogyed9BSWxNsViyGwl%2BokkpNy4SXNg9T03wIhAL%2BLAJRITn8u%2FHGgbNNJ96JjYpC88pd5ggpomUocgiCoKowFCGQQAhoMNTQzNjI5NzQyMjAyIgyALOdEX7EsIFBQAVkq6QR9dntlwAfIBQd9BTNAMNBoM7Kf%2BpbNx8%2FNFIPyzqO8%2FLrVfzaXsm08umqLSgkHyFeP7KInIp2P8r5JNr9hBHddwEtuuIbmodkdvvwjW0alUpUKObiOdygumqgCGCUkanTx5%2BkLcMWo1%2B2xBCCEHRyxjHPg2OC04Ffq5hmCIpkSD0lWJLNUEzplxhU1WthKjpmGgbOIC%2BqfxhIB5Fcznl9S9UZ7u%2Boe%2FaV3go96fKg4BuPSk2SCYjO5I82vDakVesUp70Dohf%2FeQhotjZZszhqI9Raj%2FJpoUaTwo%2FamZpY6a2o1fZKoWhsgwFBUF790Eo6QrL7E%2Bu5kNGe%2B1L6K17jBy4He4yzSqSudatrXEWOnFmkoXMC2SECwP6SurJV%2BP6vpb3C9ClGcdaXaGgucN2O%2F8AZ9qFpEKxjpRx3jcwWRD5AJHPi0FaSPgm2oUmLwMVl4%2FpdJ6TOjIWgH7amA7oUPJGGhzfb7seAQJdSGQYumP3kga0MZa1RI1KEjJJkxVhuheMyb4jRnxb7tNqTz5D0%2BPXfXMl2Phxi6cE2e9D3EdW%2BibcUQh%2FxwXO1Vu9dAfxvsbUZqEHC%2Fu4Av0JUyDSTlCnNVAPeAnTbbbNhBdy7YLVC%2FhWQPaUiO8OObXDXrREstvTjRU0rm5%2BD9Vq6eBpQit3AWjI79akiN6Ge0zfgXC41mPIdPUmTavyBodJEoF%2BvQI6YquLNpdOayRJRqN9ac8SlbDt2bKf%2Fk2Fk17OxfZ7go0Uj1mtdSQDWVQZUq9rMceAkxUdTCGyeaNWbSe%2FW%2FUG8VhQSOy8eGxqooDiOtn7w%2FR1sZCMXiSTCkzu2yBjqZAUC%2BluMphRmzX6Do6CxQGguteLfGHvhgfbOe3J4jTF0J4OuLjnPKnZ5v4njhdYk3MU3cwjpyu0KAgDb5lRFWsSKcJicPlTKKb%2Fib4aObA%2BjMGGHFKsSD3bzU1lrYfqX8QoX1idKq6eb1dSU8dsLv%2FmoyZ4MlfAZ%2FShKXKbMK6MMjWovsjb5og9HMJPnMomxwTMpXLYklqeiSgg%3D%3D&X-Amz-SignedHeaders=host&X-Amz-Signature=69de7d9b7898eefc2c40d5f7d19ea47b82e6114f14aff62024d6f18524af43f4\n",
      "fetched\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: 'store/csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbulk_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m, in \u001b[0;36mbulk_fetch\u001b[0;34m(url, destFileRef)\u001b[0m\n\u001b[1;32m     80\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfetched\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m     83\u001b[0m       zip_ref\u001b[38;5;241m.\u001b[39mextractall(csv_path)\n\u001b[1;32m     84\u001b[0m       \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracted files to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/tensortown/lib/python3.8/zipfile.py:1251\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: 'store/csv'"
     ]
    }
   ],
   "source": [
    "bulk_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.Timestamp('2024-05-23') \n",
    "\n",
    "tickers_data = nasdaqdatalink.get_table('SHARADAR/TICKERS', table = ['SEP'], paginate=True)\n",
    "sep_path = '\"moneybot/dbms/store/csv/SHARADAR_SEP.csv'\n",
    "actions_path = '\"moneybot/dbms/store/csv/SHARADAR_ACTIONS.csv'\n",
    "daily_path =  \"moneybot/dbms/store/csv/SHARADAR_DAILY.csv\"\n",
    "\n",
    "sep_data = pd.read_csv(\n",
    "    sep_path,\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=[\"date\", \"ticker\"]\n",
    ").sort_index()\n",
    "\n",
    "actions_data = pd.read_csv(\n",
    "    actions_path,\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=[\"date\", \"ticker\"]\n",
    ").sort_index()\n",
    "\n",
    "daily_data = pd.read_csv(\n",
    "   daily_path,\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=[\"date\", \"ticker\"]\n",
    ").sort_index()\n",
    "# Find the maximum date\n",
    "max_date = daily_data.index.get_level_values('date').max()\n",
    "\n",
    "# Filter the DataFrame to include only the rows with the maximum date\n",
    "daily_data = daily_data.loc[max_date]\n",
    "\n",
    "# Reset the index to make 'ticker' a column\n",
    "daily_data = daily_data.reset_index()\n",
    "\n",
    "# Set the index to 'ticker' only\n",
    "daily_data = daily_data.set_index('ticker')\n",
    "\n",
    "\n",
    "tickers_data[\"last_sale\"] = tickers_data[\"ticker\"].apply(\n",
    "    lambda ticker: sep_data.loc[(date, ticker), 'close'] if (date, ticker) in sep_data.index else None\n",
    ")\n",
    "tickers_data['marketcap'] = tickers_data['ticker'].apply(\n",
    "    lambda x: daily_data.loc[x, 'marketcap'] if x in daily_data.index else None\n",
    ")\n",
    "# tickers_data[\"marketcap\"] = daily_data[\"marketcap\"].astype(float)\n",
    "tickers_data[\"ipoyear\"] = tickers_data[\"firstpricedate\"].dt.year.astype(float)\n",
    "\n",
    "stock_data = tickers_data[\n",
    "    [\n",
    "        \"ticker\",\n",
    "        \"name\",\n",
    "        \"last_sale\",\n",
    "        \"marketcap\",\n",
    "        \"ipoyear\",\n",
    "        \"sector\",\n",
    "        \"industry\"\n",
    "    ]\n",
    "]\n",
    "stock_data = stock_data.dropna(subset=['marketcap'])\n",
    "stock_data.dropna(subset=[\"ticker\"], inplace=True)\n",
    "\n",
    "\n",
    "print(stock_data.info())\n",
    "\n",
    "stock_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put('sharadar/sep/stocks', stock_data.set_index('ticker'))\n",
    "    print(\"DataFrame stored successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
