{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import nasdaqdatalink\n",
    "# from requests import Session\n",
    "# from requests_cache import CacheMixin, SQLiteCache\n",
    "# from requests_ratelimiter import LimiterMixin, MemoryQueueBucket\n",
    "# from pyrate_limiter import Duration, RequestRate, Limiter\n",
    "# class CachedLimiterSession(CacheMixin, LimiterMixin, Session):\n",
    "#     pass\n",
    "\n",
    "# session = CachedLimiterSession(\n",
    "#     limiter=Limiter(RequestRate(2, Duration.SECOND*5)),  # max 2 requests per 5 seconds\n",
    "#     bucket_class=MemoryQueueBucket,\n",
    "#     backend=SQLiteCache(\"yfinance.cache\"),\n",
    "# )\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PROXY_SERVER=os.getenv(\"PROXY_SERVER\")\n",
    "api_key=os.getenv(\"NASDAQ_DATA_LINK_API_KEY\")\n",
    "nasdaqdatalink.ApiConfig.api_key = api_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STORE = Path('store/test.h5')\n",
    "csv_path = Path('store/csv') \n",
    "if not csv_path.exists():\n",
    "    csv_path.mkdir() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Whole Sharadar Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select a table:\n",
      "1. 1: SHARADAR/TICKERS\n",
      "2. 2: SHARADAR/ACTIONS\n",
      "3. 3: SHARADAR/DAILY\n",
      "4. 4: SHARADAR/SEP\n",
      "5. 5: SHARADAR/SP500\n",
      "6. 6: SHARADAR/SF1\n",
      "Fetching SHARADAR/SP500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# enter the Sharadar table you would like to retrieve \n",
    "def display_menu():\n",
    "    options = [\n",
    "        \"1: SHARADAR/TICKERS\",\n",
    "        \"2: SHARADAR/ACTIONS\",\n",
    "        \"3: SHARADAR/DAILY\",\n",
    "        \"4: SHARADAR/SEP\",\n",
    "        \"5: SHARADAR/SP500\",\n",
    "        \"6: SHARADAR/SF1\",  \n",
    "    ]\n",
    "    \n",
    "    print(\"Please select a table:\")\n",
    "    for i, option in enumerate(options, 1):\n",
    "        print(f\"{i}. {option}\")\n",
    "    \n",
    "    choice = input(\"Enter the number of your choice: \")\n",
    "    return int(choice)\n",
    "\n",
    "def select_table():\n",
    "    tbl = ''\n",
    "    choice = display_menu()\n",
    "    if choice == 1:\n",
    "        print(\"Fetching SHARADAR/TICKERS\")\n",
    "        tbl = 'TICKERS'\n",
    "    elif choice == 2:\n",
    "        print(\"Fetching SHARADAR/ACTIONS\")\n",
    "        tbl = 'ACTIONS'\n",
    "    elif choice == 3:\n",
    "        print(\"Fetching SHARADAR/DAILY\")\n",
    "        tbl = 'DAILY'\n",
    "    elif choice == 4:\n",
    "        print(\"Fetching SHARADAR/SEP\")\n",
    "        tbl = 'SEP'\n",
    "    elif choice == 5:\n",
    "        print(\"Fetching SHARADAR/SP500\")\n",
    "        tbl = 'SP500'\n",
    "    elif choice == 6:\n",
    "        print(\"Fetching SHARADAR/SF1\")\n",
    "        tbl = 'SF1'\n",
    "    else:\n",
    "        print(\"Invalid choice, please try again.\")\n",
    "        select_table()\n",
    "    return tbl\n",
    "    \n",
    "\n",
    "table = select_table()\n",
    "\n",
    "destFileRef = f'store/csv/SHARADAR_{table}.csv.zip'\n",
    "# optionally add parameters to the url to filter the data retrieved,\n",
    "#  as described in the associated table's\n",
    "#  documentation, eg here: https://www.quandl.com/databases/SF1/documentation/getting-started\n",
    "url = 'https://www.quandl.com/api/v3/datatables/SHARADAR/%s.json?qopts.export=true&api_key=%s' % (table, api_key) \n",
    "\n",
    "def bulk_fetch(url=url, destFileRef=destFileRef):\n",
    "  version = sys.version.split(' ')[0]\n",
    "  if version < '3':\n",
    "    import urllib2\n",
    "    fn = urllib2.urlopen\n",
    "  else:\n",
    "    import urllib\n",
    "    fn = urllib.request.urlopen\n",
    "\n",
    "  valid = ['fresh','regenerating']\n",
    "  invalid = ['generating']\n",
    "  status = ''\n",
    "  \n",
    "  while status not in valid:\n",
    "    Dict = json.loads(fn(url).read())\n",
    "    last_refreshed_time = Dict['datatable_bulk_download']['datatable']['last_refreshed_time']\n",
    "    status = Dict['datatable_bulk_download']['file']['status']\n",
    "    link = Dict['datatable_bulk_download']['file']['link']\n",
    "    print(status)\n",
    "    if status not in valid:\n",
    "      time.sleep(60)\n",
    "\n",
    "  print('fetching from %s' % link)\n",
    "  zipString = fn(link).read()\n",
    "  f = open(destFileRef, 'wb')\n",
    "  f.write(zipString)\n",
    "  f.close()\n",
    "  print('fetched')\n",
    "\n",
    "  # extract the zip file\n",
    "  with zipfile.ZipFile(destFileRef, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "        print(f'Extracted files')\n",
    "  \n",
    "  # rename file \n",
    "  for extracted_file in csv_path.iterdir():\n",
    "        if extracted_file.is_file() and extracted_file.name.startswith(f'SHARADAR_{table}'):\n",
    "            new_file_name = destFileRef.replace('.zip', '')\n",
    "            extracted_file.rename(new_file_name)\n",
    "            print(f'Renamed {extracted_file.name} to {new_file_name}') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert/store stock metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = pd.Timestamp('2024-05-23') \n",
    "\n",
    "tickers_data = nasdaqdatalink.get_table('SHARADAR/TICKERS', table = ['SEP'], paginate=True)\n",
    "sep_path = '\"moneybot/dbms/store/csv/SHARADAR_SEP.csv'\n",
    "actions_path = '\"moneybot/dbms/store/csv/SHARADAR_ACTIONS.csv'\n",
    "daily_path =  \"moneybot/dbms/store/csv/SHARADAR_DAILY.csv\"\n",
    "\n",
    "sep_data = pd.read_csv(\n",
    "    sep_path,\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=[\"date\", \"ticker\"]\n",
    ").sort_index()\n",
    "\n",
    "actions_data = pd.read_csv(\n",
    "    actions_path,\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=[\"date\", \"ticker\"]\n",
    ").sort_index()\n",
    "\n",
    "daily_data = pd.read_csv(\n",
    "   daily_path,\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=[\"date\", \"ticker\"]\n",
    ").sort_index()\n",
    "# Find the maximum date\n",
    "max_date = daily_data.index.get_level_values('date').max()\n",
    "\n",
    "# Filter the DataFrame to include only the rows with the maximum date\n",
    "daily_data = daily_data.loc[max_date]\n",
    "\n",
    "# Reset the index to make 'ticker' a column\n",
    "daily_data = daily_data.reset_index()\n",
    "\n",
    "# Set the index to 'ticker' only\n",
    "daily_data = daily_data.set_index('ticker')\n",
    "\n",
    "\n",
    "tickers_data[\"last_sale\"] = tickers_data[\"ticker\"].apply(\n",
    "    lambda ticker: sep_data.loc[(date, ticker), 'close'] if (date, ticker) in sep_data.index else None\n",
    ")\n",
    "tickers_data['marketcap'] = tickers_data['ticker'].apply(\n",
    "    lambda x: daily_data.loc[x, 'marketcap'] if x in daily_data.index else None\n",
    ")\n",
    "# tickers_data[\"marketcap\"] = daily_data[\"marketcap\"].astype(float)\n",
    "tickers_data[\"ipoyear\"] = tickers_data[\"firstpricedate\"].dt.year.astype(float)\n",
    "\n",
    "stock_data = tickers_data[\n",
    "    [\n",
    "        \"ticker\",\n",
    "        \"name\",\n",
    "        \"last_sale\",\n",
    "        \"marketcap\",\n",
    "        \"ipoyear\",\n",
    "        \"sector\",\n",
    "        \"industry\"\n",
    "    ]\n",
    "]\n",
    "stock_data = stock_data.dropna(subset=['marketcap'])\n",
    "stock_data.dropna(subset=[\"ticker\"], inplace=True)\n",
    "\n",
    "\n",
    "print(stock_data.info())\n",
    "\n",
    "stock_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    store.put('sharadar/sep/stocks', stock_data.set_index('ticker'))\n",
    "    print(\"DataFrame stored successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn csv into dataframe and delete .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Find, read, and delete the CSV file\n",
    "for extracted_file in csv_path.iterdir():\n",
    "    if extracted_file.is_file() and extracted_file.suffix == '.csv':\n",
    "        df = pd.read_csv(extracted_file)\n",
    "        print(f'Read CSV file: {extracted_file.name}')\n",
    "        os.remove(extracted_file)\n",
    "        print(f'Deleted file: {extracted_file.name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
