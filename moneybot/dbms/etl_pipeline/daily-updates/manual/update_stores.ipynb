{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "import nasdaqdatalink\n",
    "from requests import Session\n",
    "from requests_cache import CacheMixin, SQLiteCache\n",
    "from requests_ratelimiter import LimiterMixin, MemoryQueueBucket\n",
    "from pyrate_limiter import Duration, RequestRate, Limiter\n",
    "class CachedLimiterSession(CacheMixin, LimiterMixin, Session):\n",
    "    pass\n",
    "\n",
    "session = CachedLimiterSession(\n",
    "    limiter=Limiter(RequestRate(2, Duration.SECOND*5)),  # max 2 requests per 5 seconds\n",
    "    bucket_class=MemoryQueueBucket,\n",
    "    backend=SQLiteCache(\"yfinance.cache\"),\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "PROXY_SERVER=os.getenv(\"PROXY_SERVER\")\n",
    "api_key=os.getenv(\"NASDAQ_DATA_LINK_API_KEY\")\n",
    "nasdaqdatalink.ApiConfig.api_key = api_key\n",
    "\n",
    "session = requests_cache.CachedSession('yfinance.cache')\n",
    "session.headers['User-agent'] = 'moneybot/1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharadar Daily Data Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_data = nasdaqdatalink.get_table('SHARADAR/TICKERS', table = ['SEP'], paginate=True)\n",
    "\n",
    "print(tickers_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_metrics = pd.read_csv(\n",
    "    \"data/SHARADAR_DAILY.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    "    index_col=[\"date\", \"ticker\"]\n",
    ").sort_index()\n",
    "# Find the maximum date\n",
    "max_date = daily_metrics.index.get_level_values('date').max()\n",
    "\n",
    "# Filter the DataFrame to include only the rows with the maximum date\n",
    "daily_metrics = daily_metrics.loc[max_date]\n",
    "\n",
    "# Reset the index to make 'ticker' a column\n",
    "daily_metrics = daily_metrics.reset_index()\n",
    "\n",
    "# Set the index to 'ticker' only\n",
    "daily_metrics = daily_metrics.set_index('ticker')\n",
    "\n",
    "print(daily_metrics.head())\n",
    "print(daily_metrics.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# yfinance Minute Data Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # enter the Sharadar table you would like to retrieve \n",
    "def display_menu():\n",
    "    options = [\n",
    "        \"1: SHARADAR/TICKERS\",\n",
    "        \"2: SHARADAR/ACTIONS\",\n",
    "        \"3: SHARADAR/DAILY\",\n",
    "        \"4: SHARADAR/SEP\",\n",
    "        \"5: SHARADAR/SP500\",\n",
    "        \"5: SHARADAR/SF1\",  \n",
    "    ]\n",
    "    \n",
    "    print(\"Please select a table:\")\n",
    "    for i, option in enumerate(options, 1):\n",
    "        print(f\"{i}. {option}\")\n",
    "    \n",
    "    choice = input(\"Enter the number of your choice: \")\n",
    "    return int(choice)\n",
    "\n",
    "def select_table():\n",
    "    choice = display_menu()\n",
    "    if choice == 1:\n",
    "        print(\"Fetching SHARADAR/TICKERS\")\n",
    "        table = 'TICKERS'\n",
    "    elif choice == 2:\n",
    "        print(\"Fetching SHARADAR/ACTIONS\")\n",
    "        table = 'ACTIONS'\n",
    "    elif choice == 3:\n",
    "        print(\"Fetching SHARADAR/DAILY\")\n",
    "        # Add your code for Option 3 here\n",
    "    elif choice == 4:\n",
    "        print(\"Fetching SHARADAR/SEP\")\n",
    "        # Add your code for Option 4 here\n",
    "    elif choice == 5:\n",
    "        print(\"Fetching SHARADAR/SP500\")\n",
    "        # Add your code for Option 4 here\n",
    "    elif choice == 6:\n",
    "        print(\"Fetching SHARADAR/SF1\")\n",
    "        # Add your code for Option 4 here\n",
    "    else:\n",
    "        print(\"Invalid choice, please try again.\")\n",
    "        select_table()\n",
    "\n",
    "table = select_table()\n",
    "\n",
    "destFileRef = f'temp/{table}_download.csv.zip' # enter the destination that you would like the retrieved data to be saved to\n",
    "url = 'https://www.quandl.com/api/v3/datatables/SHARADAR/%s.json?qopts.export=true&api_key=%s' % (table, api_key) # optionally add parameters to the url to filter the data retrieved, as described in the associated table's documentation, eg here: https://www.quandl.com/databases/SF1/documentation/getting-started\n",
    "\n",
    "def bulk_fetch(url=url, destFileRef=destFileRef):\n",
    "  version = sys.version.split(' ')[0]\n",
    "  if version < '3':\n",
    "    import urllib2\n",
    "    fn = urllib2.urlopen\n",
    "  else:\n",
    "    import urllib\n",
    "    fn = urllib.request.urlopen\n",
    "\n",
    "  valid = ['fresh','regenerating']\n",
    "  invalid = ['generating']\n",
    "  status = ''\n",
    "  \n",
    "  while status not in valid:\n",
    "    Dict = json.loads(fn(url).read())\n",
    "    last_refreshed_time = Dict['datatable_bulk_download']['datatable']['last_refreshed_time']\n",
    "    status = Dict['datatable_bulk_download']['file']['status']\n",
    "    link = Dict['datatable_bulk_download']['file']['link']\n",
    "    print(status)\n",
    "    if status not in valid:\n",
    "      time.sleep(60)\n",
    "\n",
    "  print('fetching from %s' % link)\n",
    "  zipString = fn(link).read()\n",
    "  f = open(destFileRef, 'wb')\n",
    "  f.write(zipString)\n",
    "  f.close()\n",
    "  print('fetched')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(api_key[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STORE = Path('store/assets.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5494 entries, 0 to 5493\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   ticker     5494 non-null   object \n",
      " 1   name       5494 non-null   object \n",
      " 2   last_sale  5494 non-null   float64\n",
      " 3   marketcap  5494 non-null   float64\n",
      " 4   ipoyear    5494 non-null   float64\n",
      " 5   sector     5490 non-null   object \n",
      " 6   industry   5490 non-null   object \n",
      "dtypes: float64(3), object(4)\n",
      "memory usage: 300.6+ KB\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# sp500 = nasdaqdatalink.get_table('SHARADAR/SP500', action='current')\n",
    "# tickers = sp500['ticker'].tolist()\n",
    "df = pd.read_csv('STOCK_META_DATA.csv')\n",
    "df.info()\n",
    "\n",
    "# Set the ticker column as the index and sort by the index\n",
    "df.set_index('ticker', inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "tickers = df.index.to_list()\n",
    "max_length = max(len(s) for s in tickers)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing 'yf/minute/us_equity/prices' from the store\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    # Remove the specific key if it exists\n",
    "    if 'yf/minute/us_equity/prices' in store:\n",
    "        store.remove('yf/minute/us_equity/prices')\n",
    "        print(\"Removed existing 'yf/minute/us_equity/prices' from the store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: store/assets.h5\n",
      "/yf/minute/sp500/prices            frame_table  (typ->appendable_multi,nrows->703229,ncols->9,indexers->[index],dc->[datetime,ticker])\n"
     ]
    }
   ],
   "source": [
    "with pd.HDFStore(DATA_STORE) as store:\n",
    "    print(store.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_store_history_data():\n",
    "    sleep_time = 2.5\n",
    "    batch_start_time = time.time()\n",
    "    # for t in tickers[n1:n2]:\n",
    "    with pd.HDFStore(DATA_STORE) as store:\n",
    "        for t in tickers:\n",
    "            try:\n",
    "                df = yf.Ticker(t).history(\n",
    "                   interval=\"1m\",\n",
    "                   start=\"2024-05-02\",\n",
    "                   end=\"2024-05-04\",\n",
    "                   prepost=True,\n",
    "                   proxy=PROXY_SERVER,\n",
    "                   keepna=True\n",
    "                )\n",
    "                if df.empty:\n",
    "                    print(f\"No data found for {t}\")\n",
    "                    continue\n",
    "                \n",
    "                df.index = df.index.tz_localize(None)\n",
    "                # Add the ticker column\n",
    "                df['ticker'] = t\n",
    "\n",
    "                # Set the multi-index\n",
    "                df.set_index(['ticker', df.index], inplace=True)\n",
    "                df.index.names = ['ticker', 'datetime']\n",
    "\n",
    "                # Store the dataframe\n",
    "                if 'yf/minute/us_equity/prices' not in store:\n",
    "                    store.put('yf/minute/us_equity/prices', df[:0], format='table', min_itemsize={'ticker': max_length})\n",
    "                store.append('yf/minute/us_equity/prices', df, format='table', min_itemsize={'ticker': max_length})\n",
    "                print(f'added {t} to store')\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {t}: {str(e)}\")\n",
    "                if \"429\" in str(e):\n",
    "                    if sleep_time<3.0:\n",
    "                        sleep_time += 0.5\n",
    "                        print(\"429 error detected, delaying to respect rate limit...\")\n",
    "                    time.sleep(10.0)\n",
    "            time.sleep(sleep_time)\n",
    "    batch_end_time = time.time()\n",
    "    batch_total_time = batch_end_time - batch_start_time\n",
    "    print(f\"Batch ran for {batch_total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "fetch_and_store_history_data()\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Program ran for {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stooq Hour Data Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
